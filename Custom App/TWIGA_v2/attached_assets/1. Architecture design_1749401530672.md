


# TWIGA Split Platform Architecture Blueprint

## Overview

TWIGA will operate two parallel but fully integrated product layers:

1. **TWIGA Web (Full Platform):** Desktop-first full-featured platform targeting institutional investors, VCs, family offices, and professional funds.
2. **TWIGA Swipe (Tinder Mode Mobile App):** Mobile-first simplified swipe experience targeting angel investors, scouts, accelerators, and smaller ticket investors.

---

## TWIGA Web

- Full-fledged dashboard, advanced search filters, sectoral SDG matching.
- Detailed Twiga Rating visibility.
- Multi-step company onboarding and assessment.
- Full contact request workflow, chats, data room, deal closure modules.
- Subscription and credit-based monetization model.
- Admin portal for taxonomy management, user management, and analytics.
- Infrastructure as previously designed: REST API backend, RDS database, containerized deployment on AWS.

---

## TWIGA Swipe

- Mobile app (iOS/Android, using React Native or Flutter).
- Simplified investor onboarding (preference collection).
- Daily batch of company cards to swipe.
- Swipe Right triggers credit deduction and sends match request.
- Limited filtering options (sector, geography, stage).
- Push notification enabled (Firebase Cloud Messaging).
- Uses existing backend APIs with lightweight extensions.
- Separate API endpoints for:
    - Swipe Queue Generator
    - Swipe Tracker Table
    - Credit enforcement on swipe
- Shares authentication, payment, chat, match-making flows from core TWIGA.
- Focused on virality, fast engagement, and growth funnel feeding into full TWIGA Web.

---

## Backend Extensions for Swipe

- Swipe Queue Service: Generates daily swipe decks.
- Swipe Tracker Table: Records companies shown/swiped.
- Credit consumption: Swipe right invokes unlock + credit deduction.
- Minimal changes to Match, Chat, Data Room modules (fully compatible).

---

## Development Roadmap

1. Build Swipe API layer (2-3 weeks)
2. Develop Mobile App frontend (4-6 weeks)
3. Internal beta testing (2 weeks)
4. External limited beta (2-3 weeks)
5. Public app store release

---

## Strategic Advantage

- Unified backend simplifies maintenance.
- Two distinct investor experience pathways.
- Scales both serious institutional users and mobile-first fast movers.
- Creates organic funnel for larger pipeline growth.
- Mobile product optimized for engagement, viral features, and fast activation.

---

This architecture will allow TWIGA to serve both professional institutional investment workflows and fast-moving angel-style discovery workflows with shared infrastructure, data integrity, and secure operations.

Proposed Web Application Architecture for Twiga (Match4Impact Project)

Overview

Twiga (developed by Match4Impact) is a novel web platform designed to connect sustainability-oriented startups and projects (“Zebras”) with impact-focused investors (“Seekers”) ￼. It serves as a two-sided marketplace that bridges companies in need of funding with investors looking for ventures aligned to their impact criteria. A core innovation of Twiga is its unique taxonomy and rating system: companies complete a comprehensive sustainability questionnaire, and Twiga generates a standardized Twiga Rating (encompassing both disclosure completeness and performance scores, nicknamed “Zebra scores”) based on financial and non-financial criteria ￼. This taxonomy aggregates the best ESG/impact standards and yields a classification that all parties can respect as a common metric ￼. The rating and taxonomy enable precise matching – investors can reliably gauge a company’s sustainability profile and compare it to others.

In addition to assessment, Twiga provides tools for matchmaking and collaboration. Investors get an intelligent search interface to filter and discover suitable companies, and the platform offers a secure environment for both sides to connect, communicate, and ultimately formalize investment deals ￼. In essence, Twiga spans the entire process: from company assessment and visibility, through investor search and filtering, to facilitated introductions, due diligence, and deal closing – all within one integrated web application. The architecture must support these diverse functions while maintaining scalability and flexibility for future growth (the platform is expected to evolve with new features and increasing user volume) ￼.

Key Requirements
	•	Dual User Portals (Companies & Investors): The system must cater to two distinct user types – impact-oriented companies and professional investors – each with dedicated experiences. Companies (the “Zebras”) need to register and showcase their business projects, while investors (“Seekers”) need tools to discover and evaluate those projects ￼. The interface and workflows for each should be tailored to their goals (e.g. companies input data and monitor interest; investors search and initiate contact).
	•	Comprehensive Assessment Questionnaire & Rating: Twiga incorporates an extensive sustainability assessment for companies, based on a custom taxonomy of ESG and impact criteria. Companies fill out a detailed questionnaire covering multiple modules (e.g. governance, environmental impact, social factors, financials), which produces a Twiga Rating comprised of a disclosure score and one or more “Zebra scores” for performance ￼ ￼. This scoring mechanism is core to the platform – it standardizes how projects are evaluated against best practices. The system must support dynamic calculation of these scores and presentation of the results (e.g. percentile rankings, badges like “high match” for top scores) to interested investors. (For example, a company selecting several UN SDGs will receive multiple Zebra scores – one for each relevant impact area – allowing targeted comparison in those categories ￼.)
	•	Intelligent Search & Matchmaking: Investors require a powerful search tool to find companies that meet their specific criteria. The platform should enable filtering companies by various attributes such as industry sector, target SDGs, stage, location, and of course Twiga Rating thresholds. Using the taxonomy classification, Twiga can precisely match investors’ preferences with suitable companies ￼. For instance, if an investor is interested in sectors like “Water” or “Decarbonization”, they can filter for companies aligned with those SDGs; the system will then show matching companies and reveal their corresponding Zebra scores in those categories ￼. This requires a robust querying capability (potentially with a search index) to sift through profiles and return ranked results. The search results should give enough overview (e.g. basic company info and an indication of match quality) to prompt investors to explore further.
	•	Profile Visibility & Access Control: Twiga employs a granular access model for company information. Basic profile data for companies (e.g. name, sector, tagline) may be visible to all registered investors, but the full details – including the Twiga Rating breakdown and detailed questionnaire responses – are gated behind a paywall or credit system ￼. Investors must spend credits or payments to unlock a company’s complete profile (“See more” action) ￼. Thus, the architecture needs to enforce content gating: the backend should deliver summary data by default, and serve full data only to authorized requests (after checking the user’s subscription/credit balance). This protects sensitive company information and creates a revenue model around data access.
	•	Contact and Match Workflow: The platform facilitates introductions through a controlled matchmaking workflow. An interested investor, after viewing a company’s full profile, can send a contact request to that company through Twiga ￼. The company user will receive a notification in their portal and can choose to accept or decline the request. No direct contact info is exchanged until the company accepts – this mutual opt-in defines a “Match”. The system should automatically expire unanswered requests after a set period (e.g. 7 days) to prevent stale open requests ￼, resetting the opportunity if the company is unresponsive. This workflow requires tracking request status, sending timely notifications, and enforcing expiration rules.
	•	In-Platform Communication (Chat): Once a Match is made (i.e. the company accepts an investor’s contact request), Twiga provides a private chat channel for the two parties ￼. This in-app messaging allows them to discuss details of the opportunity in a secure, logged environment without leaving the platform. The chat needs to function in real-time or near-real-time (for a smooth user experience) and be accessible only to the matched investor-company pair. Business rules from the documentation indicate the chat should be time-bound – for example, if there is no activity in the chat for a few days after opening, the system may automatically close that chat to encourage prompt engagement ￼. Therefore, the architecture should include a real-time communication service (such as WebSocket-based chat) with the ability to deactivate the channel based on inactivity or time limits.
	•	Secure Data Room for Due Diligence: As conversations progress, the investor and company may need to exchange documents (financial statements, pitch decks, legal documents, etc.). Twiga offers a secure data room feature for this purpose ￼. The data room is an online repository where the matched parties can upload and download confidential files. Access to the data room is granted only when both sides agree to proceed to due diligence, and it likely involves an additional fee or premium access level (as noted in the business model) ￼. The architecture must ensure that files in a data room are encrypted and only accessible to the specific match participants. It should also enforce an expiration: if the deal does not go forward, the shared documents and any related data should be wiped after a grace period to protect confidentiality ￼. This requires secure file storage and a mechanism to destroy or archive data upon session termination.
	•	Deal Closure and Outcome Tracking: If the investor and company decide to proceed with an investment, the platform should allow them to mark the opportunity as a successful Deal. Twiga would then record that a deal was closed (and according to the business model, trigger a final transaction or commission fee) ￼. The architecture should support a “deal confirmation” step where terms can be summarized and both parties acknowledge the deal within the system. Once a deal is confirmed, that match’s status changes to a closed deal; if no deal is reached, the match might simply expire and be closed without further action, with the platform ensuring all interim data (chat logs, files) are deleted for privacy ￼. This means the backend needs to manage a state machine for matches (from initial request -> active chat -> data room -> deal or no deal) and handle state transitions and cleanup accordingly.
	•	Payment & Credit System: Twiga’s features are monetized through subscriptions and microtransactions, so the system must include a robust payment and credit accounting subsystem. Companies are expected to subscribe (e.g. monthly fee) to fully participate and see their detailed Twiga Rating and be part of the Twiga network ￼. Investors operate on a credit model: for example, an investor’s membership may come with a few credits included, and each time they want to unlock a company’s full profile they spend a credit ￼. Additional credits can be purchased in packages (with some monthly cap to encourage careful selection) ￼. Other actions for investors might also incur fees/credits – for instance, initiating a chat with a company might deduct a credit or have a fee ￼, and accessing a data room has a significant fee split between the parties ￼. Finally, closing a deal triggers a commission fee for both sides ￼. The platform therefore needs integration with a payment gateway to handle credit purchases and subscriptions, and it must maintain ledger records of each user’s credit balance, payments, and usage. All paid actions should check against the user’s balance/entitlements before proceeding (e.g. “does the investor have a credit available to view this profile?”). Ensuring transactional consistency in these operations is crucial (so users are charged correctly and can’t bypass the system).
	•	Scalability, Security & Compliance: Given the potentially global scope of Twiga (targeting companies and investors worldwide) ￼, the architecture must be scalable and designed for high availability. The documentation emphasizes that the platform should be able to grow with new functionalities and users without major redesign ￼. This implies using a modular and extensible design, and deploying on infrastructure that can handle increasing load (through vertical scaling or horizontal scaling with load balancers). Security is paramount: Twiga will store sensitive business information and facilitate confidential communications, so all data should be encrypted in transit (HTTPS for all web/mobile traffic) and at rest (especially in the data room). Strict role-based access control is needed to segregate company vs. investor vs. admin permissions. The system should implement proactive security measures – for example, web application firewalls and intrusion detection systems to fend off attacks and abuse ￼ – ensuring a “secure environment” for users to interact (a point highlighted in the project’s vision ￼). Additionally, compliance with data protection regulations (GDPR and similar, given personal and possibly financial data involved) must be accounted for in design. Audit logs of user activities (profile views, file accesses, etc.) might be required for security and compliance auditing. Finally, the platform should have robust monitoring and failover mechanisms to keep downtime to a minimum (enterprise investors will expect high reliability).

Recommended Architecture Diagram

Architecture Overview: The Twiga platform will be structured in a classic multi-tier architecture, separated into presentation, application, and data layers. For clarity, we can imagine the architecture in terms of three main facets: front-end applications, a back-end API server, and the supporting services (database, storage, external integrations). At a high level, the design includes multiple front-end clients (for different user roles) all communicating with a central backend via secure APIs ￼:
	•	On the client side, there will be two primary web applications: the Marketplace app for Zebras and Seekers (the main user-facing site), and an Administration app for Twiga staff (back-office). Both are delivered as web UIs (and potentially mobile-friendly or as mobile apps in the future). These front-ends are responsible for providing a rich user experience but contain no business logic; they rely entirely on calling the back-end services for data and actions.
	•	The server side consists of a RESTful API backend that encapsulates all core business logic. According to the proposal, this will be an independent API service connected to the central database ￼. It serves as the single entry point for all client requests – handling everything from authentication to querying ratings to processing match actions. This backend is built to manage different access levels and functionalities for all types of users (companies, investors, and admins) within one system ￼. Internally, the backend is organized into modules or services corresponding to the major domains (user management, assessment, search, matchmaking, etc., described in the next section).
	•	The data and integration layer includes the primary application database as well as external services the platform relies on. Twiga will use a central database to persist all its data (user accounts, profiles, questionnaire results, messages, etc.). Additionally, the architecture incorporates a file storage system for the data room (e.g. a secure cloud storage bucket) and potentially a search engine or indexing service to support the investor search queries (for performance when filtering by many criteria). The platform also integrates with external APIs: notably a Payment Gateway for handling credit card transactions and subscriptions, and an email/SMS service for notifications (for example, to alert a user of a new contact request or to verify emails). These external integrations are orchestrated by the backend when needed (e.g. the backend calls the payment API when a user purchases credits).

In diagrammatic form, one could imagine the front-end (browser or app) on the left, making HTTPS calls to the Twiga API server (middle layer). The API server then interacts with the database and other services on the right. There are distinct UI portals for Zebras/Seekers vs. Super Admin, but they converge on the same API (with role-based authorization) ￼. Real-time features like chat might introduce a WebSocket connection component: for instance, the clients could connect to a WebSocket server (either part of the API server or a separate service) to enable instant messaging. This real-time server would also interface with the main backend (for authentication and message delivery logic). The architecture also includes background workers or scheduled jobs – for tasks such as expiring inactive chats, sending reminder emails, or cleaning up old data rooms. The Recommended Architecture Diagram (described above) emphasizes modularity and separation of concerns, ensuring that each piece of the system can scale or be replaced independently as needed.

(Assumption: We propose using a containerized deployment for the backend and possibly the front-end (e.g. Docker containers), orchestrated by a cloud service or Kubernetes. This detail was not explicitly in the documentation, but it aligns with scalability and portability best practices.)

Frontend Components

The front-end will be implemented as responsive web applications (with the potential for native mobile apps later). There are three main front-end interfaces corresponding to user roles:
	•	Zebra (Company) Web Portal: This is the interface for startup founders or companies seeking funding. It allows a company to register and create a profile, guiding them through the Twiga assessment questionnaire. Key features of the Zebra portal include a multi-step form for inputting all required data for the Twiga taxonomy (with the ability to save progress and continue later), as well as a dashboard to review their results. Initially, after completing the questionnaire, a company can see a basic profile or “pre-sheet” with their provided data, but not the full analytical Twiga Rating ￼ until certain conditions are met. If the company opts to fully join the platform (by paying the subscription fee), they gain access to a comprehensive dashboard that includes their Twiga Rating and all the detailed parameters derived from the taxonomy ￼. The company portal also includes features to manage incoming investor requests: e.g., viewing a list of interested investors, notifications of new contact requests, and the ability to accept or decline those requests. Once a match is made, the company user will use the integrated Chat interface in the portal to communicate with the investor. They can also upload documents to the Data Room through this interface when needed. In summary, the Zebra portal is a secure logged-in area where a company user can manage their profile, see their impact scores, and engage with potential investors.
	•	Seeker (Investor) Web Portal: This is the interface for investors or financiers looking for projects. It provides a search and browse experience across the database of companies. The home view might be a searchable directory of companies with summary info (name, sector, maybe a few indicative metrics). Investors can use filters to narrow down companies by region, sector, SDG focus, size, etc., and set preferences to find matches. The portal supports viewing a company’s public profile snippet, and importantly, the action to unlock the full Twiga report of a company. When an investor chooses to “View More” on a particular company, the front-end will trigger the purchase/credit deduction workflow and then display the full profile including the Twiga Rating details ￼. The UI should reflect if the investor has enough credits and show a confirmation before spending a credit. After unlocking, the investor can see all the data points the company provided (except possibly certain sensitive data hidden from investors entirely). If interested, the investor can then click a “Contact Company” button to send a match request ￼. The Seeker portal will show the status of all such requests the investor has made (pending, accepted = in chat, or expired/declined). For any accepted match, the investor gains access to the private Chat window within the app to talk directly to that company’s team. The investor portal also includes screens for managing the investor’s profile (their name, organization, interests) and a payment section to buy more credits or subscriptions. Since investors have to make an initial payment to join (e.g. €199 for 3 credits as per the business model) ￼, the portal will include onboarding steps for payment. The search results and company profile pages will emphasize the Twiga scores (e.g. showing a “match strength” or percentile) to help investors prioritize which opportunities to pursue. Overall, the investor front-end is geared towards discovery and evaluation, followed by transaction flows (paying for data access, initiating contact).
	•	Admin Dashboard: Twiga administrators have a separate back-office interface to manage the platform. This dashboard is accessible only to internal staff (Super Admin users) and provides controls for moderation and configuration. Key functions include taxonomy management – admins can create or edit the questionnaire modules, questions, scoring weights, etc., which is crucial as standards evolve. The admin dashboard also allows viewing and managing user accounts (both companies and investors): for example, verifying company registrations, resetting passwords, or removing profiles that violate terms. Admins can monitor platform activity through this interface, such as tracking the number of matches made, active deals, and usage of credits. They may also have tools to intervene in a match if necessary or to generate reports on impact metrics. The admin front-end will likely be a simpler, data-heavy UI with tables and forms for CRUD operations on the various entities. According to the technical proposal, a dedicated back-office front-end will be developed for Twiga admins ￼, separate from the main marketplace UI, to compartmentalize administrative functions. This ensures regular users do not access these sensitive operations and that admins have a tailored experience for managing the system.

All front-end components will be built with modern web technologies, ensuring a responsive and intuitive user experience. We assume the use of a framework like React, Angular, or Vue for a Single Page Application (SPA) approach, which will communicate with the backend via AJAX/HTTPS (and via WebSockets for the chat). The SPAs will handle client-side routing for different views (e.g. a route for the questionnaire form, a route for search results, etc.) and maintain state such as the user’s login session (likely via a token stored after authentication). The front-end will also implement form validations (especially for the lengthy questionnaire), dynamic elements like progress bars (for profile completion), and possibly data visualization (e.g. graphs of the company’s performance on various metrics for their dashboard). Given the need for a professional look to attract investors, a clean UI design and ease of use are critical. The proposal notes that Twiga’s design should be “attractive and innovative” to suit the target audience ￼, so the front-end work will include a thoughtful UX/UI design phase.

Backend Components

The backend is the heart of the Twiga platform, implementing all business logic and rules. It is envisioned as a RESTful API service with a modular structure. Key backend components (which could be separate services in a microservice architecture, or modules in a single application initially) include:
	•	User Authentication & Authorization: Responsible for user accounts, login/auth flow, and role management. This module will handle secure storage of credentials (password hashing, etc.), issuance of authentication tokens or sessions upon login, and enforcement of permissions. It ensures that each API request is authenticated and that users can only access resources they’re entitled to (e.g., an investor cannot call admin APIs, a company can only modify its own profile, etc.). It also supports features like password resets, email verification, and possibly multi-factor authentication if needed for extra security (especially for investors handling payments). This component will integrate with third-party identity verification as needed and logs authentication events for security audits.
	•	Company Profile & Assessment Management: This module covers all functionality around companies (Zebras) filling out information and getting rated. It includes sub-components for the Questionnaire Logic – serving the list of questions (taxonomy) to the front-end, validating inputs, and storing responses. It also encompasses the Twiga Rating Engine: once the data is submitted, this engine computes the scores according to the Twiga methodology (e.g. calculating the disclosure completeness percentage and the various Zebra performance scores) ￼. The rating engine might apply weighting formulas and lookup reference standards (as defined in Twiga’s taxonomy configuration) to generate a consistent score. The results are then stored and made available for retrieval. This module must be designed to accommodate updates to the questionnaire and scoring criteria (perhaps reading from a configuration or database-stored schema, so that administrators can update the taxonomy without re-deploying code). It also should allow companies to update their information periodically (triggering a re-calculation of scores). In essence, this component manages the entire lifecycle of a company profile: creation, data input, score calculation, and providing a summarized profile (with or without scores depending on context). It will likely interact with the database tables for Companies, Questions, Answers, and Scores (see Database Design).
	•	Investor Profile & Search Service: This handles the investor-side logic, particularly the search and filtering capabilities. It manages storing any investor preferences (though investors might not need to pre-save preferences, they could just filter on the fly, but the system could allow saving favorite filters or alerts). More critically, this service implements the querying of the company database based on criteria. It likely builds database queries (or search index queries) that filter companies by certain attributes (e.g., match specific SDGs or sectors) and possibly sorts or ranks them by relevance (for example, giving higher rank to those with higher Zebra scores in the selected category). For performance and advanced filtering, we might integrate a search engine like Elasticsearch; however, even a well-indexed relational query could suffice initially. This service will also enforce that an investor only sees appropriate data: e.g., the search results might return only summary info for each company. If an investor wants full details, this service checks their credit balance and either grants access or prompts payment. Essentially, the Search Service coordinates with the Profile/Rating data to deliver the investor the information they request, in line with Twiga’s matchmaking logic ￼. It may also include a recommendation system in the future (suggesting companies that are “high match” for an investor based on their behavior or profile).
	•	Matchmaking & Notification Module: This component manages the contact request flow and system notifications. It defines the state machine for a match: an investor initiates a request to contact a company, the request is recorded as pending, the company is notified (e.g., via a notification alert in their portal and optionally an email), and a timer is started for expiry ￼. If the company accepts within the allowed window, the module transitions the state to “matched” and triggers creation of a chat channel for the pair ￼. If the company declines or the request times out, the state goes to “rejected/expired” and no connection is opened (and possibly the investor’s credit for that contact is not charged or is refunded according to policy). This module must ensure that an investor cannot directly contact a company without acceptance – i.e., enforce the rule that no communication channel opens until a mutual match. It also cleans up expired requests (removing or archiving them, and possibly notifying the investor that it wasn’t accepted). The notification aspect involves sending out alerts: it could use an email service or push notifications to inform a company “Investor X wants to connect” or inform an investor “Company Y accepted your request”. Internally, it likely leverages a job scheduler for the 7-day expiration logic (so a background job will mark requests as expired and notify the investor if no action). This module also could handle system messages like reminders (“You have X days left to respond”) or other platform announcements.
	•	Real-Time Chat Service: Once a match is made, Twiga’s private messaging service enables direct communication. This could be implemented as part of the main backend or as a specialized service optimized for real-time communication. A likely approach is using WebSockets (or a similar push mechanism) to allow a live chat experience. The Chat Service will authenticate users (only allow participants of a confirmed match to connect to that match’s chat room) and then facilitate exchanging messages. It should log chat messages in the database (for compliance or later reference, at least until the deal is closed or the chat expires). However, given confidentiality, the system plans to delete these logs if no deal is reached, so the service should support ephemeral data handling – e.g., flagging messages for deletion when a match is closed without a deal ￼. The chat service might also enforce the time limit for inactivity: e.g., it could have a mechanism to automatically close or archive the chat if, say, 7 days pass with no new messages after opening ￼. Implementing this might involve the chat service sending a warning to both parties and then disabling new messages if inactive. Technically, this service will integrate with the main backend (to be aware of match creation/closure events) and possibly use a publish/subscribe system or in-memory message brokers for scalability if many concurrent chats occur. Security here is key: all messages should be encrypted in transit and access-controlled by match ID.
	•	Secure Data Room / File Storage Service: For document exchange, the backend will include a file handling component. This service manages file uploads, storage, and access permissions for the Data Room feature. Likely, when a match decides to enter due diligence, this module will create a secure space (e.g., generate a unique folder or encryption context for that match). Files uploaded by either party are stored (for example, on an AWS S3 bucket or Google Cloud Storage, with server-side encryption). The service saves metadata in the database (file name, link, which match it belongs to, who uploaded it, etc.). When someone tries to download a file, the service checks that the user is part of the match and has an active data room session. We might generate pre-signed URLs for downloads to ensure direct access is controlled. This module must also implement cleanup: when a match is concluded, especially if it does not result in a deal, the system should purge the files for that match ￼. This could be done via an automated job that deletes the storage objects and corresponding DB records. Additional considerations include virus scanning for uploads (to prevent malware) and version control if needed (though likely not, as it’s meant for simple exchange). Since data rooms are a paid, time-limited feature, this service also interacts with the payment module to verify that the fee has been paid before opening access ￼.
	•	Payments & Credits Management: This component handles all monetization aspects. It integrates with external payment gateways (such as Stripe, PayPal, or another processor) to accept credit card payments or bank transfers from users. Key functionalities include: processing the company’s subscription payments (recurring billing), handling investor purchases of credit packs, and charging one-off fees for actions like opening a chat or a data room, or final deal commissions. Internally, the module maintains each user’s balance or entitlement: for companies, whether their subscription is active; for investors, how many credits they have remaining this month, etc. When an investor attempts a paid action (view profile, start chat), this module is consulted to verify and deduct credits accordingly ￼. It should record every transaction in a ledger for accounting transparency (e.g., credit deduction when an investor views a profile, or split invoicing for the data room fee shared by investor and company ￼). The payment module also needs to handle payment webhooks (notifications from the payment gateway confirming a charge, etc.) to update user statuses. Security is crucial here: sensitive payment data should not be stored on Twiga servers (the gateway handles card info), and all payment-related interactions must be over SSL and follow compliance (PCI DSS, etc.). This module will also power any billing interface on the front-end (so that users can see their purchase history, receipts, current plan status, etc.). By encapsulating all this, we ensure the rest of the system can simply query “has user X paid for Y?” without dealing with payment details directly.
	•	Administration & Analytics: In addition to user-facing features, the backend includes admin-only capabilities. Admin users (through the admin dashboard) can invoke APIs to manage content: e.g., Taxonomy Administration (CRUD for questions, criteria, scoring weights), user management (activate/deactivate accounts, assign roles), and viewing platform metrics. There might be an analytics sub-component which aggregates data such as number of companies by sector, average scores, number of matches made, conversion rates, etc., giving the Twiga team insight into platform performance. This could be done via separate reporting queries or even a small data warehouse if needed, but initially, simple queries on the main database might suffice. Admin functions also include content moderation (ensuring company profiles or investor profiles meet guidelines) – the backend might provide endpoints for admins to edit or remove inappropriate content. Since the admin operations can be sensitive (e.g., editing the scoring model), these endpoints are extra-protected (accessible only by admin role, possibly with additional verification). The technical proposal explicitly mentions a “dashboard de gestión y administración” for entities ￼, which corresponds to these admin capabilities. The backend supports this by providing all necessary endpoints for the admin front-end.

Each of these components will be implemented with scalability and maintainability in mind. Initially, they might reside in a single monolithic codebase (for rapid development), but with clear separation of concerns so that specific services (like chat or search) could be scaled out or isolated later. The backend will enforce all the business rules outlined in the Key Requirements (for example, ensuring a contact request cannot be created if an investor has insufficient credits, or ensuring a company can’t see its own Twiga score until subscription payment is verified, etc.). By structuring the backend into these logical components, the team can develop and test each piece relatively independently (for instance, testing the rating calculation module separately from the chat module). It also aligns with the proposal’s guidance that the development be extensible for new functionalities in the future ￼, as new modules can be added without rewriting the core.

Tech Stack (Assumption): The documents do not mandate a specific technology stack, but a common approach for such a platform could be to use a Node.js/Express or Python/Django (or Flask/FastAPI) for the REST API server, paired with a relational database (for transactional integrity). The real-time chat might use Node.js with WebSocket libraries (Node is well-suited for realtime) or a service like Socket.io. Alternatively, a Java/Spring or C#/.NET backend could be used if the team prefers those ecosystems – the key is to choose a robust framework that supports REST, database ORM, and integrates with WebSocket or similar for chat. In any case, the backend will be built following REST principles (and possibly GraphQL for flexible queries, though not indicated, REST is more straightforward here) ￼. JSON will be used for data exchange between front-end and back-end.

Database Design

Twiga’s data model will encompass multiple entities corresponding to the real-world concepts in the platform. A relational database (such as PostgreSQL or MySQL) is a strong choice, given the need for complex queries (especially for search and filtering) and the transactional consistency required for things like credit deductions. The main tables and their relationships could be designed as follows:
	•	Users: A table for user accounts, storing login credentials (email, password hash) and profile info. It will include a role field or type (e.g., “company”, “investor”, or “admin”) to distinguish user types. Basic contact info, organization name (for investors) or personal name, etc., would be here. If one company can have multiple user logins (e.g., multiple team members), we might separate the concept of user vs company entity, but for simplicity one user = one company is assumed in this phase. (Alternatively, a separate Company table can hold the organization details and link to a user owner.)
	•	Companies (Zebras): A table for the company profiles, linked to the Users table (if separate) or combined if one-to-one. This stores company-specific info: company name, description, sector, location, size, etc. It also includes fields like whether the company’s profile is complete, whether they have paid subscription (could be a flag or linked to a Subscription table), and timestamps (registration date, last updated). This table would also have foreign keys or relations to the questionnaire results and scores that belong to the company.
	•	Taxonomy / Questions: Tables to represent the Twiga questionnaire structure. One table might define Questionnaire Modules (categories like Governance, Environmental, Social, Economic, etc.), and another table for Questions themselves, with fields like question text, type (multiple-choice, numeric, text), which module it belongs to, and possibly a maximum point value or weight. These tables allow the questionnaire to be dynamic and modifiable by admins. For example, a question entry could say: “Module = Environmental, Question = ‘Do you measure your carbon footprint annually?’, Type = Yes/No, Points = 5 (if yes)”. The taxonomy may also require tables for reference data (like lists of SDGs or sectors that a company can select, which we might store in lookup tables).
	•	Survey Responses / Answers: A table capturing each company’s answer to each question. It would have foreign keys: CompanyID, QuestionID, and fields for the answer value (could be text, number, option id, etc.). For multi-select questions, this might involve a separate association table (e.g., CompanyID, QuestionID, OptionID for each selected option). This data is used to calculate scores. It can be a large table (since one company will generate many rows, one per question answered). To optimize, one could also store JSON blobs of answers module-wise, but fine-grained rows give more query flexibility. This table, combined with the Questions table, allows us to compute the disclosure score (e.g. count of answered questions) and evaluate key responses for Zebra score.
	•	Scores / Ratings: To avoid recalculating on every view, the computed Twiga scores for each company can be stored in a Scores table. This table might have columns for each main score: e.g., DisclosureScore, OverallZebraScore, and perhaps a normalized OverallPercentile. However, since each company can have multiple Zebra scores (one per SDG or focus area they selected) ￼, we might design a separate CompanySectorScore table: each entry linking a Company to a specific category (like an SDG or sector) and containing that company’s Zebra score (percentile) in that category. For example, Company123 – SDG6 (Clean Water) – Score 85%. This way, when an investor filters by “Water sector”, we can quickly retrieve the relevant score ￼. Another approach is to have a wide table with many columns for each category’s score, but that’s less flexible as categories can change. Storing scores separately also allows showing labels like “High Match” or “Recommended” based on ranges ￼. In summary, we’ll have a data structure to record each company’s current scores resulting from the latest assessment.
	•	Matches (Investor-Company Connections): A table to represent the contact requests and matches. Each record would contain an InvestorID (link to Users, where role=investor), a CompanyID, a status (e.g., “Requested”, “Accepted”, “Declined”, “Expired”, “ChatOpened”, “DealClosed”), timestamps for request, accept, etc., and maybe a small field for notes or initial message. This table is central to managing the investor-company interactions. Only when status becomes “Accepted/Matched” do we allow chat and data room. It also can store a reference to a “match chat room ID” if needed for the chat service to route messages. If a match results in a deal, that outcome could be flagged here as well (and possibly a separate Deal table for details of the deal could reference this).
	•	Chat Messages: A table (or collection, if using something like Mongo for chat, but likely SQL is fine) logging the messages exchanged in each match’s chat. Fields include MatchID (link to the Matches table), sender (Investor or Company user ID), timestamp, and message content (probably text). Given that chats may be deleted after a deal/no-deal, this table’s data is transient. We might implement a routine to purge messages when a match is closed without a deal (or after X days of inactivity). Still, during the active chat, storing messages here enables features like loading chat history when a user opens the chat window. If the volume is high or real-time performance is an issue, this could be moved to an in-memory store or specialized chat service, but initially SQL will work with proper indexes.
	•	Data Room / Files: A set of tables to manage file metadata. One table (e.g., DataRoomSessions) could represent an active data room session per match, with fields for MatchID, start time, expiration time, and paid status. Another table (DataRoomFiles) would list the files uploaded in those sessions: each with an ID, the MatchID, uploader (user), file name, file path or key in cloud storage, upload timestamp. We do not store the file content in the database (files will be stored in the cloud or server file system), but we keep references. These records facilitate listing files for the users and verifying access. Once a session expires or is closed, the system can iterate through related DataRoomFiles entries and delete the actual files and then these records.
	•	Transactions / Payments: Tables to record payments, subscriptions, and credit allocations. For example, a Payments table for all payment transactions (with fields: user, date, amount, type (subscription vs credits vs fee), and an external transaction ID from the payment gateway, status, etc.). A Subscriptions table could track company subscriptions (user, start date, end date, current status), unless we handle that simply via the Payments table and a flag on the user. For credits, we could either use a simple counter field on the investor’s profile (like credits_remaining) that gets updated, and/or a CreditTransactions log table that records each credit addition and subtraction (for auditing). For instance, if an investor buys 3 credits, an entry is added; if they view a profile costing 1 credit, an entry is added (with negative amount) and the current balance is decremented. Having a log ensures we can reconstruct or verify balances if needed. The final deal commission could also appear as a transaction record (even if handled offline, it would be good to log that a deal occurred with X fee).
	•	Audit & Miscellaneous: We might include tables for audit logs (if needed for compliance, e.g. logging every time an investor views a profile could be stored), and for system configurations (like feature flags or settings). Also, if Twiga plans a content/news feed (they mentioned a “network of contacts and interests, news and projects” community aspect ￼), there could be tables for posts or comments, though that’s speculative. For now, the above covers the main data entities.

All tables will use primary keys (often auto-incrementing IDs) and foreign keys to enforce relationships (e.g., Match references Investor and Company IDs; Answers reference Company and Question). Using a relational schema ensures consistency (for example, a Match cannot exist for a company that doesn’t have a profile in the Companies table). We will also add appropriate indexes: for example, on the Answers table (by CompanyID and maybe question category) to speed up score calculations, on the Match table (by InvestorID or CompanyID to quickly fetch a user’s interactions), and on some text fields for search (or we use full-text search if needed for company descriptions). If search requirements become complex (like keyword search in company descriptions or mission statements), we might integrate an external search engine, but initially we can rely on well-chosen DB indices.

Finally, data retention policies will be implemented at the database level for certain tables: chat messages and data room files will be purged when appropriate; personal data might be anonymized or deleted upon user deletion (to comply with GDPR’s right to be forgotten, for instance). Regular backups of the database will be taken to prevent data loss.

APIs

The system exposes a set of RESTful (JSON-based) APIs for all functionalities. These APIs are the bridge between the front-end applications and the backend components described above. Below is an overview of the major API endpoints/groups and their roles:
	•	Authentication & User APIs: Endpoints for signing up and logging in. For example:
	•	POST /api/auth/register – Create a new user (with role specified). There could be separate endpoints or a field indicating if it’s a company or investor registration. Company registration might require additional fields up front (company name, etc.).
	•	POST /api/auth/login – Authenticate a user and return a token (if using JWT) or establish a session.
	•	POST /api/auth/logout – Invalidate session/token.
	•	POST /api/auth/forgot-password and POST /api/auth/reset-password – support for account recovery.
These endpoints ensure secure authentication. They will likely implement rate limiting and verification (e.g., email verification flow via POST /api/auth/verify-email). All subsequent API calls will require an Authorization header (JWT or session cookie) to verify the user’s identity and role.
	•	Company (Zebra) APIs: Endpoints for companies to manage their profile and assessment.
	•	GET /api/taxonomy – Retrieve the questionnaire structure (modules, questions, options) so the front-end can render the form. (Admins could have a corresponding POST/PUT to update the taxonomy.)
	•	POST /api/companies – Submit a new company profile or assessment answers. This could be a single endpoint that takes the entire questionnaire payload. Alternatively, it might be broken down by section (e.g., /api/companies/{companyId}/answers with module data).
	•	GET /api/companies/{companyId} – Retrieve the company’s own profile and results. A company user would get the full detail of their profile (including Twiga Rating if available). An investor hitting this endpoint would only get full detail if they have unlocked it (the backend will check permissions/credits).
	•	PUT /api/companies/{companyId} – Allow updating profile info or re-submitting answers (maybe limited edits).
	•	GET /api/companies/{companyId}/dashboard – (If not covered by the above) specifically to get the Twiga Rating and dashboard metrics for the company. This might be used for the company’s own view of their metrics (accessible if they have a subscription) ￼. If the company has not paid, the API could return a partial response or a flag indicating they need to upgrade.
	•	These endpoints ensure a company user can input and retrieve their data. Proper authorization checks mean that company A cannot access company B’s profile through these endpoints (unless admin).
	•	Investor (Seeker) & Search APIs:
	•	GET /api/companies – List or search companies. This endpoint would accept query parameters for filters (e.g., ?sector=Energy&country=Kenya&sdg=6). It returns a list of companies matching the criteria, each with summary info (e.g., name, location, maybe a brief description, and possibly an indicator of Twiga score range like high/medium match). This is the core search functionality ￼. We might paginate results and allow sorting by certain keys (like most recent or highest score).
	•	GET /api/companies/{companyId}/public – (if needed) retrieve the public-facing snippet of a single company. Alternatively, the search endpoint’s results might suffice for that.
	•	POST /api/matches – Initiate a contact request. The investor calls this with the target company’s ID (and maybe a message). The backend checks that the investor has the right to do so (e.g., they have unlocked the profile and perhaps have not exceeded some limit). It then creates a Match record in “Requested” state and triggers a notification to the company.
	•	GET /api/matches – List the current user’s match records. For an investor, this would list all requests they’ve made and their status (and maybe any incoming requests, though typically companies receive requests, investors send them). For a company, this lists incoming investor requests (pending or past).
	•	PUT /api/matches/{matchId} – Possibly used by the company to accept or decline a request. (Or we can have a more specific endpoint, e.g., POST /api/matches/{matchId}/accept and /api/matches/{matchId}/decline to make it clear.)
	•	GET /api/companies/{companyId}/detail – When an investor wants to view the full detail of a company, this endpoint can be used. It will check if the investor has permission (i.e., they have a credit to spend or have spent one for this company already). If allowed, it returns the complete profile including Twiga Rating data. If not, it might return a 402 Payment Required or a flag indicating “payment needed”. In practice, the front-end would likely call a purchase endpoint first, then call this.
	•	Credits purchase might be under Payment API (see below). The search and match endpoints ensure investors can find and connect with companies, enforcing the business rules along the way.
	•	Chat APIs: The chat is likely handled via WebSocket for real-time, but there may be REST endpoints for ancillary functions:
	•	GET /api/chat/token (or similar) – to initiate a WebSocket connection, sometimes an authenticated token or handshake via REST is used. This could ensure only authorized users connect to the real-time server.
	•	GET /api/chat/history?matchId=X – to retrieve past messages for a chat (if we want to load history when opening a chat window, instead of storing all in client memory). The backend would return recent messages from the Chat Messages table.
	•	WebSocket channels: The system will have an established pattern like a channel or room per match. Messages sent over the socket by one user will be broadcast to the other party and also persisted via the Chat Service module. The architecture might use a messaging broker for scaling (like Redis Pub/Sub or a dedicated service), but that’s internal. From an API perspective, the WebSocket server is another endpoint (like wss://api.twiga.com/chat). The front-end will subscribe after a match is made and user navigates to chat.
	•	There could also be a POST /api/chat/close for forcibly closing a chat (maybe invoked by the system or admin after inactivity or at match end).
	•	Data Room APIs:
	•	POST /api/datarooms – Open a data room for a given match. Likely called when one party requests due diligence. It might require both parties’ action, but the first call could set a flag and send a notification to the other to confirm. Alternatively, this could be invoked automatically when both click “enter data room” on their UI.
	•	POST /api/datarooms/{roomId}/files – Upload a file to the data room. This could be a direct upload endpoint that handles the file (multipart form-data) and stores it. Or the backend might issue a pre-signed URL so the front-end can upload directly to cloud storage, then call another endpoint to register the file.
	•	GET /api/datarooms/{roomId}/files – List files available in that data room (for displaying to the users).
	•	GET /api/datarooms/{roomId}/files/{fileId} – Download a specific file (this could redirect to a signed URL or stream the file).
	•	POST /api/datarooms/{roomId}/close – Close the data room (could be done when a deal is finalized or if the session expires). This might trigger deletion of files. In practice, this could also be automatic (no explicit API call by users).
	•	The Data Room endpoints must enforce that only the investor or company of the associated match can access them, and possibly that the data room fee has been paid. If the fee payment is handled as part of opening the room, POST /api/datarooms would check and charge if not already done.
	•	Payment APIs: These endpoints manage purchasing and transactions:
	•	POST /api/payments/subscribe – Company subscribes to the platform. This might integrate with a payment form or redirect to a third-party checkout. The endpoint would initiate the process and record pending subscription.
	•	POST /api/payments/buy-credits – Investor purchases a credit package. The request might include how many or which package, or it might be predetermined (e.g., always 3 credits for a fixed price as per business model). This will similarly engage the payment gateway and update the investor’s credit count on success.
	•	Webhooks: POST /api/payments/webhook – to receive notifications from the payment provider (e.g., Stripe webhook calls here when a payment succeeds). The backend will then confirm the transaction and update the relevant records (activate subscription, increment credits, etc.).
	•	GET /api/payments/transactions – Users (or admins) could fetch a history of transactions for record-keeping.
	•	POST /api/payments/deal – Possibly used when marking a deal closed to handle the commission payment. Or it could be automatically triggered by the system when both parties mark the deal done. This would charge both parties as per the agreed formula ￼. The endpoint would likely only be called after user confirmation of a deal.
	•	Admin APIs: Only accessible to admin accounts (with appropriate auth):
	•	GET /api/admin/overview – summary stats for dashboard.
	•	GET /api/admin/companies (and /investors) – list all users with filtering, perhaps to approve or review them.
	•	PUT /api/admin/companies/{id} – update or moderate a company profile (e.g., change status, or edit details if requested).
	•	DELETE /api/admin/companies/{id} – remove a company (e.g., if fraudulent or by request).
	•	POST /api/admin/taxonomy and PUT /api/admin/taxonomy/{questionId} – create or edit taxonomy questions/structure. Admin can add new questions or modify weights. These calls update the Questions/Modules tables accordingly ￼.
	•	GET /api/admin/matches / GET /api/admin/deals – view all matches or deals on the platform.
	•	POST /api/admin/announcement – possibly to broadcast a message to all users (if a news feature).
	•	These endpoints allow full control over the platform data and configurations. They will be protected by checks ensuring the user is an admin. The actions here might also be audited (logged separately).

Response formats and error handling: All APIs will return JSON. On success, typically an object or list of objects is returned. On failure or invalid input, an error JSON with a code and message is returned. For example, if an investor tries to view a profile without credits, the GET /api/companies/{id}/detail might return a 403 Forbidden with a message like “Insufficient credits” or a custom error code, so the front-end can prompt for purchase. The design should include clear error codes for scenarios such as unauthorized, validation errors on input (e.g., required fields missing in the questionnaire), payment failures, etc.

Security for APIs: We will use HTTPS for all endpoints to encrypt data in transit. Authentication could be managed via JWT tokens that the front-end attaches to the Authorization header of each request. The backend will verify the token and also the user’s roles/permissions for that endpoint. Important operations like payment or deal confirmation might require extra verification (perhaps re-checking a user’s session or using anti-CSRF tokens if using cookies). Rate limiting should be applied to prevent abuse of public endpoints (e.g., login brute force protection, search throttling to avoid scraping). The inclusion of WAF (web application firewall) means many malicious requests (SQL injection attempts, etc.) will be filtered before hitting the API ￼. We will also ensure that user inputs are properly sanitized and validated on the server side (never trust only client validation).

Deployment Infrastructure

The Twiga web application will be deployed on a scalable, secure cloud infrastructure. The development proposal explicitly mentions using cloud environments (AWS, GCP, or Azure) for hosting the platform ￼, so we will outline an AWS-oriented setup as an example (the approach on GCP/Azure would be analogous):
	•	Hosting & Compute: The backend API will run on cloud servers. We can containerize the application using Docker and deploy it on a managed container service (e.g., AWS Elastic Container Service or Kubernetes on EKS). This allows easy scaling: we can run multiple instances of the API server behind a load balancer to handle concurrent users. The load balancer (AWS ALB or ELB) will distribute incoming HTTPS requests to the API instances and also terminate SSL (offloading TLS). For the real-time chat component, we may run it as a part of the same API service or as a separate service (either way, ensuring sticky sessions or using a message broker to handle WebSocket scaling). The infrastructure will have separate environments for development, staging, and production deployments, each with its own isolated resources (to allow testing without affecting prod).
	•	Database: We will use a managed relational database service such as AWS RDS (with PostgreSQL or MySQL). A single primary instance will host the production database, with read-replicas if needed for scaling read operations. RDS provides automated backups, point-in-time recovery, and easy scaling of resources, which align well with Twiga’s reliability needs. The DB will be placed in a private subnet (not directly accessible from the internet), and the application servers will communicate with it over the internal network. We will enforce least privilege – the app will connect with credentials that only have necessary privileges, and security groups will restrict access (e.g., only the app servers can talk to the DB on the required port). As usage grows, the DB instance size can be increased, and for high availability, we can enable multi-AZ deployment (so a standby replica is kept in sync in another availability zone for failover).
	•	Storage: For the Data Room file storage, we’ll leverage a service like Amazon S3 (Simple Storage Service) or Azure Blob Storage / Google Cloud Storage. These services are ideal for storing binary files securely. Each file upload from the app will be stored in a protected bucket; we can use bucket policies or pre-signed URLs to ensure only authorized access. S3 also provides server-side encryption (AES-256) for data at rest, and transfers to/from S3 can be forced over HTTPS. We will organize the file storage by match or company, and possibly enable lifecycle rules to auto-delete files after a certain time (e.g., 90 days) to align with the data room expiration policy. S3 is highly scalable and durable, which means we don’t have to worry about capacity for file storage as the platform grows.
	•	Content Delivery: The front-end static assets (HTML, CSS, JS of the SPA) can be served via a CDN (Content Delivery Network) like CloudFront or Azure CDN. This improves load times for users globally and offloads traffic from the servers. We can have an S3 bucket or equivalent to host the static site files, fronted by the CDN for caching. If the front-end is server-side rendered or not static, we’d instead deploy it similarly to the backend (as a Node.js app or using a service like Vercel for a React app). In our architecture, likely the front-end will be a static build (since it’s a SPA) that can be deployed on a CDN easily.
	•	Networking & Security: We will set up the cloud network (VPC in AWS) with public subnets for load balancers and private subnets for application servers and databases. Security groups and firewall rules will ensure that:
	•	The load balancer accepts incoming traffic on HTTPS (port 443) and HTTP (80 redirect to 443) and forwards to the app servers on the internal ports (e.g., 5000).
	•	The app servers can communicate with the DB on its port (5432 for Postgres, for instance) and with the S3 endpoints, etc.
	•	The database is not accessible from the internet at all.
	•	We will enforce TLS everywhere: the LB will have an SSL certificate (likely from Let’s Encrypt or AWS Certificate Manager for the domain match4impact.com or twiga.match4impact.com) ￼, ensuring secure HTTPS for users. For internal service-to-service communication, we can also use TLS or rely on the VPC’s security.
	•	A Web Application Firewall (WAF) will be enabled on the load balancer or using a service like AWS WAF. This will provide an additional layer of defense by filtering common attack patterns (SQL injection, XSS, bots). The proposal specifically recommends a WAF and IDS (Intrusion Detection System) for proactive security ￼. We can implement IDS by using a service or agent on servers that monitors logs for suspicious activity or using cloud security services that analyze traffic.
	•	The servers themselves (EC2 instances if not fully managed containers) will be hardened – only necessary ports open, regular OS patching (which the devops team will manage) ￼. With containerization, we’ll also keep base images updated and minimal.
	•	Scalability & Autoscaling: We will configure auto-scaling for the application servers. For instance, AWS Auto Scaling groups can launch more container instances when CPU or memory usage stays high, and terminate them when load decreases. This allows Twiga to handle usage spikes (e.g., a large number of investors searching or many chats happening concurrently) without manual intervention. The stateless nature of the API (assuming we store session in tokens and use shared storage for files) means any instance can serve any request, which is ideal for scaling horizontally. The chat WebSocket scaling may need sticky sessions or a stateful approach; one solution is to use a service like AWS AppSync or AWS IoT (which can handle WebSockets at scale), or ensure that the auto-scaling does not break existing WebSocket connections (possibly by having a moderate session timeout and clients auto-reconnect, or using a consistent routing based on match id). In terms of database scaling: for read-heavy loads, we can introduce read replicas and direct certain read queries (like heavy search operations) to replicas. For write scaling, sharding is not anticipated early on, but we’ll monitor performance.
	•	Monitoring & Logging: We will employ monitoring services (like CloudWatch in AWS or equivalent) to keep track of system health. This includes metrics like CPU, memory, and network I/O of servers ￼, as well as DB performance (latency, query throughput) and error rates of the application. We will set up alerts for critical conditions (e.g., high error rate, server down, DB nearing capacity) to notify the devops team. Logging is also crucial: the application will log significant events (user logins, API errors, payments, etc.) to a centralized logging system (like CloudWatch Logs or ELK stack) so that we can audit behavior and debug issues. For security, we’ll enable access logs on the load balancer (tracking all incoming requests) and possibly use an IDS/IPS system that analyzes those logs. Regular audits of these logs help detect any anomalous activities.
	•	Backup & Recovery: The database will have automated daily snapshots (with point-in-time restore enabled, typically RDS provides that) so we can recover from data corruption or accidental deletes. We might also periodically backup the questionnaire configuration and any other critical data to a secure location. File storage (S3) is inherently redundant, but for safety we could version files or keep backups of any especially important documents (though since the data room files are ephemeral, this is mostly for the case where a deal is closed – perhaps companies might want to retain the documents; we need to clarify retention policies per use case). The infrastructure configuration (like Terraform or CloudFormation templates) will be maintained as code, so we can recreate the environment if needed. In case of a disaster, having multi-AZ and even multi-region (if global users require) deployments can be considered: e.g., deploy another instance of the app in a different region and have a DR plan.
	•	Maintenance & Updates: The proposal from Bokokode indicates they will handle server administration and keep systems up-to-date ￼. We will schedule regular maintenance windows for updates to minimize impact. Using rolling deployments (containers) we can deploy new versions of the app with zero downtime (taking instances out of rotation gradually). The server OS and dependencies will be updated with security patches as they are available. A staging environment will mirror production for testing new releases. We will also enforce routine security scans (vulnerability scanning on the infrastructure and penetration testing on the application) to catch any weaknesses.
	•	Continuous Integration/Deployment (CI/CD): We will set up a CI/CD pipeline so that code changes are automatically tested and deployed. For instance, using GitHub Actions or Jenkins to run tests and then build Docker images, which are deployed to the cluster. This ensures that new features or fixes can go live quickly and reliably, supporting the project’s agile growth.

In summary, the deployment infrastructure is geared to provide a scalable, secure, and resilient environment for Twiga. By using cloud services, we offload a lot of heavy lifting (scaling, hardware management, etc.) and gain enterprise-grade security features (like built-in DDoS protection, monitoring, WAF). The architecture is aligned with the documentation’s emphasis on security (a “secure environment” for investors and companies) and reliability, ensuring Twiga can become a trusted platform for matching impact investments. All infrastructure components will be under the control of Match4Impact (even if managed by a third-party team like Bokokode, ownership remains with the company) so that the team doesn’t have to worry about the complexity of the underlying environment while still retaining full data control ￼ ￼.

(Assumptions have been made regarding specific technologies like Docker, AWS services, etc., in line with modern best practices, since the source documents did not explicitly state them. These can be adjusted to the final chosen stack, but the overall architecture remains applicable.)

Sources:
	1.	Twiga Project Long Presentation – Overview of Twiga’s purpose, features, and business model ￼ ￼ ￼ ￼ ￼.
	2.	Bokokode Proposal TWIGA V1 – Technical proposal outlining architecture, team, and infrastructure ￼ ￼ ￼ ￼ ￼.
	3.	Twiga Rating Introduction Document – Details of Twiga’s scoring methodology and selective display of scores to investors ￼ ￼.
	4.	Twiga Business Model Notes – Pricing and credit system for platform features ￼ ￼.
	5.	Match4Impact Company Info – Background on Match4Impact and Twiga’s mission ￼.
